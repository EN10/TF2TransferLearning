# -*- coding: utf-8 -*-
"""Transfer Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MNCUHb3s_HI91OX6sXGPQ_UWpYS7on4R
"""

from tensorflow.keras import *

data_root = utils.get_file(
  'flower_photos','https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
   untar=True)

IMAGE_SIZE = 224
image_generator = preprocessing.image.ImageDataGenerator(rescale=1/255)
image_data = image_generator.flow_from_directory(str(data_root), target_size=(IMAGE_SIZE, IMAGE_SIZE))

import tensorflow_hub as hub

feature_extractor_model = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4" 

feature_extractor_layer = hub.KerasLayer(
    feature_extractor_model, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), trainable=False)

model = Sequential([
  feature_extractor_layer,
  layers.Dense(image_data.num_classes)
])

model.compile(
  optimizer=optimizers.Adam(),
  loss=losses.CategoricalCrossentropy(from_logits=True),
  metrics=['acc'])

epochs = 5
history = model.fit(image_data, epochs=epochs,
                    steps_per_epoch=len(image_data))

!wget -O image.jpg https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/daisy-flower-1532449822.jpg

img = preprocessing.image.load_img('image.jpg',
                                   target_size=(IMAGE_SIZE, IMAGE_SIZE))
x = preprocessing.image.img_to_array(img)
x = backend.expand_dims(x, axis=0)

import matplotlib.pyplot as plt
plt.imshow(img)

pred = model.predict(x)
index = backend.argmax(pred)
labels = list(image_data.class_indices.keys())
print(labels[index[0]])